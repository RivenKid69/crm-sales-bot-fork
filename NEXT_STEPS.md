# Следующие шаги после установки vLLM

## Текущий статус

✅ Конфигурация обновлена на Qwen3-14B-AWQ
✅ Скрипты для запуска и тестирования созданы
✅ Документация готова
⏳ vLLM устанавливается (может занять 5-10 минут)

## Когда vLLM установится

### 1. Проверить установку

```bash
cd /home/corta/crm_sales_bot
source .venv/bin/activate
python -c "import vllm; print('vLLM version:', vllm.__version__)"
```

Должно вывести версию (например: `vLLM version: 0.13.0`)

### 2. Скачать модель Qwen3-14B-AWQ

```bash
python scripts/download_qwen3_14b.py
```

Это загрузит ~8-9 GB. Займет 5-15 минут в зависимости от скорости интернета.

### 3. Запустить vLLM сервер

```bash
./scripts/start_vllm_qwen3_14b.sh
```

Сервер запустится на `http://localhost:8000`.
Первый запуск займет 1-2 минуты (загрузка модели в VRAM).

### 4. В новом терминале: протестировать модель

```bash
cd /home/corta/crm_sales_bot
source .venv/bin/activate
python scripts/test_qwen3_14b.py
```

Это проверит:
- Здоровье сервера
- Список моделей
- Генерацию текста
- Классификацию интентов

### 5. Запустить CRM бота

Теперь ваш бот будет использовать Qwen3-14B-AWQ!

```bash
python src/main.py  # или как вы обычно запускаете
```

## Мониторинг производительности

### GPU утилизация:

```bash
watch -n 1 nvidia-smi
```

Должно показывать:
- **Memory Used**: ~12-16 GB (из 32 GB)
- **GPU Util**: 30-90% при генерации

### Скорость генерации:

Тестовый скрипт покажет реальную скорость.
Ожидается: **120-150 tokens/sec**

## Если что-то пошло не так

### vLLM не устанавливается

Проверьте процесс:
```bash
ps aux | grep pip | grep vllm
```

Если зависло, остановите (Ctrl+C) и попробуйте снова:
```bash
pip install vllm --no-cache-dir
```

### Модель не загружается

Проверьте место на диске:
```bash
df -h /home/corta
```

Нужно минимум 10 GB свободного места.

### GPU not available

Проверьте CUDA:
```bash
nvidia-smi
python -c "import torch; print('CUDA available:', torch.cuda.is_available())"
```

### Медленная генерация (<50 t/s)

1. Проверьте что используется GPU (nvidia-smi)
2. Закройте другие приложения использующие GPU
3. Убедитесь что не запущено несколько инстансов vLLM

## Документация

Полная инструкция: [docs/QWEN3_14B_SETUP.md](docs/QWEN3_14B_SETUP.md)

## Производительность

Сравнение с Qwen3-4B:

| Метрика | Qwen3-4B | Qwen3-14B | Изменение |
|---------|----------|-----------|-----------|
| Качество | Базовое | Отличное | **+40%** |
| Скорость | ~280 t/s | ~140 t/s | **-50%** |
| VRAM | ~4 GB | ~16 GB | **4x** |
| Время ответа (100 токенов) | ~0.35с | ~0.7с | **2x** |

**Вывод**: Qwen3-14B в 2 раза медленнее, но качество значительно лучше для диалогов и сложных задач.

---

## Контакты для помощи

Если возникли проблемы:
1. Проверьте логи vLLM
2. Посмотрите документацию vLLM: https://docs.vllm.ai
3. GitHub Issues: https://github.com/vllm-project/vllm/issues
